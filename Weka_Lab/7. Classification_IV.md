# WEEK-7: Remmove files and check accuracy

#### 1. [Check to see if the data shows a bias against “foreign workers” or “personal-status”. Did removing these attributes have any significantly effect? Discuss.](#section-1)

#### 2. [Another question might be, do you really need to input so many attributes to get good results? Try out some combinations, and report the Decision Tree and cross validation results for the most optimum attribute combination. Are they significantly different from results obtained with all the atrributes? Comment on your findings.](#section-2)

#### 3. [How does the complexity of the Decision Tree relate to the bias of the model for the german-credit-data? Report your findings.](#section-3)

## 1. Check to see if the data shows a bias against “foreign workers” or “personal-status”. Did removing these attributes have any significantly effect? Discuss. <a name="section-1"></a>

After choosing a classifier to evaluate the credit-g dataset with and without the attributes "foreign workers" and "personal-status", we note down the performance metrics such as precision, accuracy, recall, and F1-score, between the two datasets.
We see that after removing the attributes "foreign workers" and "personal-status" it does not have any significant effect on the model's performance. As there is no significant difference in performance metrics between the two datasets, it indicates that these attributes were not influential in the model's predictions, suggesting no evident bias.

![q1](https://github.com/00Pratik-Biswas00/College_Sixth_Semester_Labs/assets/114896796/9046b11a-7ad1-42bb-bba5-2d93e48349f4)

## 2. Another question might be, do you really need to input so many attributes to get good results? Try out some combinations, and report the Decision Tree and cross validation results for the most optimum attribute combination. Are they significantly different from results obtained with all the atrributes? Comment on your findings. <a name="section-2"></a>

![q2](https://github.com/00Pratik-Biswas00/College_Sixth_Semester_Labs/assets/114896796/ab1fb340-c825-4aca-a781-1a1249228826)

Among the above five observations the last combination give better accuracy than others.

## 3. How does the complexity of the Decision Tree relate to the bias of the model for the german-credit-data? Report your findings. <a name="section-3"></a>

The complexity of a decision tree can have a direct impact on the bias of the model. In machine learning, bias refers to the simplifying assumptions made by the model to make the target function easier to learn. Decision trees inherently have bias because they make assumptions about the data based on the features they split on.

#### 1. High Complexity:

A decision tree with high complexity, meaning it has many levels or nodes, tends to have low bias. This is because a complex decision tree can learn intricate patterns in the training data, capturing more details and nuances. However, this complexity can also lead to overfitting, where the model learns noise in the training data rather than the underlying patterns. As a result, while the bias might be low, the variance (sensitivity to small fluctuations in the training data) tends to be high.

#### 2. Low Complexity:

Conversely, a decision tree with low complexity, such as a tree with few levels or nodes, tends to have higher bias. This is because simpler trees make more assumptions about the data, potentially oversimplifying the underlying patterns. While a simple tree might generalize well to new data and avoid overfitting, it may miss important relationships present in the data, resulting in higher bias.

As the complexity of a decision tree increases, its bias tends to decrease, allowing it to capture more intricate patterns in the data. However, overly complex trees can lead to overfitting and high variance. Conversely, simpler trees tend to have higher bias but may generalize better to new data.

High complexity:

![hC](https://github.com/00Pratik-Biswas00/College_Sixth_Semester_Labs/assets/114896796/534a7c1e-9e0e-4a1a-86ea-0d0d4111c4f3)

Low complexity:

![lC](https://github.com/00Pratik-Biswas00/College_Sixth_Semester_Labs/assets/114896796/03cbffb5-df1d-48a1-bc82-597e0312d98c)

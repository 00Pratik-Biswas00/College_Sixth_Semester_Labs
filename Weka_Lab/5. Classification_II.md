# WEEK-5: Sample Programs using German Credit Data-Credit risk assessment problem


#### [1. List all the categorical (or nominal) attributes and the real-valued attributes separately; draw useful insights for the nominal and numerical attributes.](#section-1)

#### [2. What attributes do you think might be crucial in making the credit assessment? Come up with some simple rules in plain English using your selected attributes.](#section-2)

#### [3. Train a Decision tree using the complete data set as the training data. Report the model obtained after training.](#section-3)

#### [4. What % of examples can you classify correctly? (This is also called testing on the training set) why do you think cannot get 100% training accuracy?](#section-4)

#### [5. Is testing on the training set as you did above a good idea? Why or why not?](#section-5)


## 1. List all the categorical (or nominal) attributes and the real-valued attributes separately; draw useful insights for the nominal and numerical attributes.<a name="section-1"></a>

1. In the German Credit Data, which is commonly used for credit risk assessment, there are both categorical (nominal) and real-valued attributes. Let's list them separately and then draw useful insights for each type of attribute:
   
#### Categorical (Nominal) Attributes:

1. Status of existing checking account
2. Credit history
3. Purpose
4. Savings account/bonds
5. Present employment since
6. Personal status and sex
7. Other debtors/guarantors
8. Property
9. Other installment plans
10. Housing
11. Job
12. Telephone
13. Foreign worker

#### Real-valued Attributes:

1. Duration in months
2. Credit amount
3. Installment rate in percentage of disposable income
4. Present residence since
5. Age in years
6. Number of existing credits at this bank
7. Number of people being liable to provide maintenance for
8. Target (credit risk assessment results)

#### Insights for Categorical (Nominal) Attributes:

1. Distribution of Categories: This helps understand the prevalence of different categories.

2. Association with Target Variable: You can use stacked bar charts or contingency tables to see if certain categories are associated with higher or lower credit risk.

3. Multivariate Analysis: Techniques like association rule mining or correspondence analysis can help identify interesting patterns or associations between categories.

#### Insights for Real-valued Attributes:

1. Attribute Distributions: Visualize the distributions of real-valued attributes using histograms or density plots. 

2. Relationship with Target Variable: You can use box plots or scatter plots to see if there are any clear trends or patterns.

3. Correlation Analysis: This helps identify relationships and dependencies between attributes, which can be useful for feature selection or modelling.

4. Outlier Detection: Outliers may indicate errors in the data or interesting observations that warrant further investigation.

## 2. What attributes do you think might be crucial in making the credit assessment? Come up with some simple rules in plain English using your selected attributes.<a name="section-2"></a>

#### When assessing credit risk using the German Credit Data, several attributes are likely crucial for making informed assessments. Some of the key attributes that might be considered important include:

1. Credit Amount: The total amount of credit requested by the applicant.

2. Duration: The duration of the credit in months, indicating the length of time over which the loan will be repaid.

3. Purpose: The purpose for which the loan is being requested (e.g., car, furniture/equipment, education, etc.).

4. Savings Account/Bonds: The amount of savings the applicant has, which reflects their financial stability.

5. Employment Duration: The length of time the applicant has been employed at their current job.

6. Payment Status of Previous Credit: The repayment history of previous credits, which provides insight into the applicant's creditworthiness.

7. Property: The type of property owned by the applicant (e.g., real estate, car, etc.).

8. Age: The age of the applicant, can be a factor in assessing stability and reliability.

#### Based on these attributes, here are some simple rules in plain English that could be used for credit risk assessment:

1. Rule 1: Approve the credit if the credit amount is relatively small and the applicant's savings account balance is sufficient to cover potential defaults.

2. Rule 2: Reject the credit if the applicant's payment status of previous credits indicates a history of late payments or defaults.

3. Rule 3: Approve the credit if the purpose of the loan is for education or investment in property, as these purposes are likely to yield future returns.

4. Rule 4: Reject the credit if the applicant's employment duration is less than a specified threshold (e.g., 6 months), indicating unstable employment.

5. Rule 5: Approve the credit if the applicant owns valuable property (e.g., real estate) as collateral.

6. Rule 6: Reject the credit if the applicant's age is below a certain threshold (e.g., 18 years), indicating a potential lack of financial responsibility.

## 3. Train a Decision tree using the complete data set as the training data. Report the model obtained after training. <a name="section-3"></a>

![DT_J48](https://github.com/00Pratik-Biswas00/College_Sixth_Semester_Labs/assets/114896796/75373d20-36a4-4247-b83f-4af4de26dde1)


## 4. What % of examples can you classify correctly? (This is also called testing on the training set) why do you think cannot get 100% training accuracy?<a name="section-4"></a>

#### Correctly classified - 85.5%
#### Incorrectly classified - 14.5%

The training accuracy obtained (percentage of correctly classified examples on the training set) may not be 100% due to several reasons:

Overfitting: The decision tree model might have learned the training data too well, capturing noise and outliers in addition to the underlying patterns. This can result in reduced generalization ability on unseen data.

Complexity of the Model: Decision trees can create complex structures to perfectly fit the training data, especially if not constrained (e.g., max depth, min samples per leaf). This complexity can lead to overfitting.

Inherent Variability in Data: The dataset may contain inherent variability or noise that cannot be perfectly captured by the decision tree model.

## 5. Is testing on the training set as you did above a good idea? Why or why not?<a name="section-5"></a>

#### Testing on the training set, is generally not considered good practice for several reasons:

Risk of Overfitting: When you test a model on the same dataset that was used for training, you risk overfitting. Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the performance of new, unseen data. The model may perform exceptionally well on the training set but poorly generalize to new data.

Lack of Generalization: The purpose of building a machine learning model is to make accurate predictions on unseen data. Testing on the training set does not provide a realistic evaluation of how well the model will perform in practice on new, future data points. The goal is to assess the model's ability to generalize beyond the training data.

Biased Evaluation: Testing on the training set can lead to biased evaluation results. The model has already seen these data points during training, so it may be biased towards memorizing and reproducing these exact patterns rather than learning underlying patterns that can be applied to new data.

Limited Insight into Model Performance: Testing on the training set does not provide insights into how the model will perform in different scenarios or on different data distributions. It does not uncover potential weaknesses or areas where the model may struggle when faced with novel challenges.




